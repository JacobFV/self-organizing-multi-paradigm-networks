# Self Organizing Multi-Paradigm Networks

:construction: **This repository is under construction.** :construction: Stable release coming this Summer 2022.

Want to contribute? Check out the GitHub container repository [Limboid/the-artificial-ecosystem](https://github.com/Limboid/the-artificial-ecosystem) for this project.

TODO
- make single dynamic-sized linear cell
- make recurrent network with arbitrary internal structure and pooling functions (full list, random select, max, min, mean, etc)
- make a meta dynamic-time-GNN to determine the best routing/training/node growth/etc

Difference between SOMPNet and MAN:
- SOMPNet manages one end-to-end network composition of tensor transformations. No custom training. No asynchronous forward or backward pass. This might be resolved if computation was performed only one internal layer linkage at a time but I don't see how you could fit that into the nice end-to-end mould -- especially when you have output modalities changing across tasks. SOMPNet is simply a network and does not define a supervised learning objective.
- MAN definitely propagates locally (async) and maintains separate recurrent states at each node/agent. Nodes/agents define a clear interfaces which separates internal and external information. Nodes/agents are pretty much a `pt.Module`-style object (`forward`, `train`) except all inputs and outputs are dicts and the agent tells the MAN what modalities (previously port types) it accepts using a `input_modalities` (as well as `output_modalities`) attribute. Maybe RL frameworks already include obs_spec and act_spec on the agent, but a modality is something more: modality defines the structure (something composable like: set<int>, grid[X,Y,?]<float>, grid[?]<set<float>>, grid[]<float> (there is no `seq` because the models should assume increasing axis means forward for ordered grids. However in many cases, this prior is not even correct (i.e. bidirectional RNN's think in both directions so they question the effort of making `seq` separate from `grid`))) and annotated type (`common`, `vision`, `language`, `position`, etc.) of information  The MAN keeps a replay buffer for each node and they train asynchronously. Agents/nodes are expected to know what to do / what objective to optimize when training (there is an 'energy' signal to tune into).
- SOMPNet grows a little but only uses standard DL layers: linear, conv, pool, nonlinearity, etc.
- MAN uses exotic nodes/agents which almost always have a recurrent state. I want to experiment with Forier modality operations on neural fields. In fact, it should be easy to put off-the-shelf RL agents into the network (the RLNode makes them view the agent space around them as an environment).
- SOMPNet handles the aggregation function for the layers and it is handpicked or learned by the meta DTGNN. The layers output a single tensor on each output and the SOMPNet then handles aggregating multiple outputs if they exist in the network when routing. 
- MAN passes a dictionary of the full list (or empty list) of inputs for each modality to the agents. It combines the output *lists* targeted at each agent's modality. An *agent's modality* is a unique data address with an associated mailbox in the salina workspace where outputs tensors are stored. By having the ability to output multiple values for each modality, the agents can approximate distributions by sending several sample estimates to the same mailbox.